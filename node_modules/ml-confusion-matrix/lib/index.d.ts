/**
 *  Constructs a confusion matrix
 * @class ConfusionMatrix
 * @example
 * const CM = new ConfusionMatrix([[13, 2], [10, 5]], ['cat', 'dog'])
 * @param {Array<Array<number>>} matrix - The confusion matrix, a 2D Array. Rows represent the actual label and columns
 *     the predicted label.
 * @param {Array<any>} labels - Labels of the confusion matrix, a 1D Array
 */
export default class ConfusionMatrix {
    /**
     * Construct confusion matrix from the predicted and actual labels (classes). Be sure to provide the arguments in
     * the correct order!
     * @param {Array<any>} actual  - The predicted labels of the classification
     * @param {Array<any>} predicted     - The actual labels of the classification. Has to be of same length as
     *     predicted.
     * @param {object} [options] - Additional options
     * @param {Array<any>} [options.labels] - The list of labels that should be used. If not provided the distinct set
     *     of labels present in predicted and actual is used. Labels are compared using the strict equality operator
     *     '==='
     * @param {any} [options.sort]
     * @return {ConfusionMatrix} - Confusion matrix
     */
    static fromLabels(actual: Array<any>, predicted: Array<any>, options?: {
        labels?: any[] | undefined;
        sort?: any;
    } | undefined): ConfusionMatrix;
    constructor(matrix: any, labels: any);
    labels: any;
    matrix: any;
    /**
     * Get the confusion matrix
     * @return {Array<Array<number> >}
     */
    getMatrix(): Array<Array<number>>;
    getLabels(): any;
    /**
     * Get the total number of samples
     * @return {number}
     */
    getTotalCount(): number;
    /**
     * Get the total number of true predictions
     * @return {number}
     */
    getTrueCount(): number;
    /**
     * Get the total number of false predictions.
     * @return {number}
     */
    getFalseCount(): number;
    /**
     * Get the number of true positive predictions.
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getTruePositiveCount(label: any): number;
    /**
     * Get the number of true negative predictions
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getTrueNegativeCount(label: any): number;
    /**
     * Get the number of false positive predictions.
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getFalsePositiveCount(label: any): number;
    /**
     * Get the number of false negative predictions.
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getFalseNegativeCount(label: any): number;
    /**
     * Get the number of real positive samples.
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getPositiveCount(label: any): number;
    /**
     * Get the number of real negative samples.
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getNegativeCount(label: any): number;
    /**
     * Get the index in the confusion matrix that corresponds to the given label
     * @param {any} label - The label to search for
     * @throws if the label is not found
     * @return {number}
     */
    getIndex(label: any): number;
    /**
     * Get the true positive rate a.k.a. sensitivity. Computes the ratio between the number of true positive predictions and the total number of positive samples.
     * {@link https://en.wikipedia.org/wiki/Sensitivity_and_specificity}
     * @param {any} label - The label that should be considered "positive"
     * @return {number} - The true positive rate [0-1]
     */
    getTruePositiveRate(label: any): number;
    /**
     * Get the true negative rate a.k.a. specificity. Computes the ration between the number of true negative predictions and the total number of negative samples.
     * {@link https://en.wikipedia.org/wiki/Sensitivity_and_specificity}
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getTrueNegativeRate(label: any): number;
    /**
     * Get the positive predictive value a.k.a. precision. Computes TP / (TP + FP)
     * {@link https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values}
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getPositivePredictiveValue(label: any): number;
    /**
     * Negative predictive value
     * {@link https://en.wikipedia.org/wiki/Positive_and_negative_predictive_values}
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getNegativePredictiveValue(label: any): number;
    /**
     * False negative rate a.k.a. miss rate.
     * {@link https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#False_positive_and_false_negative_rates}
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getFalseNegativeRate(label: any): number;
    /**
     * False positive rate a.k.a. fall-out rate.
     * {@link https://en.wikipedia.org/wiki/Type_I_and_type_II_errors#False_positive_and_false_negative_rates}
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getFalsePositiveRate(label: any): number;
    /**
     * False discovery rate (FDR)
     * {@link https://en.wikipedia.org/wiki/False_discovery_rate}
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getFalseDiscoveryRate(label: any): number;
    /**
     * False omission rate (FOR)
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getFalseOmissionRate(label: any): number;
    /**
     * F1 score
     * {@link https://en.wikipedia.org/wiki/F1_score}
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getF1Score(label: any): number;
    /**
     * Matthews correlation coefficient (MCC)
     * {@link https://en.wikipedia.org/wiki/Matthews_correlation_coefficient}
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getMatthewsCorrelationCoefficient(label: any): number;
    /**
     * Informedness
     * {@link https://en.wikipedia.org/wiki/Youden%27s_J_statistic}
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getInformedness(label: any): number;
    /**
     * Markedness
     * @param {any} label - The label that should be considered "positive"
     * @return {number}
     */
    getMarkedness(label: any): number;
    /**
     * Get the confusion table.
     * @param {any} label - The label that should be considered "positive"
     * @return {Array<Array<number> >} - The 2x2 confusion table. [[TP, FN], [FP, TN]]
     */
    getConfusionTable(label: any): Array<Array<number>>;
    /**
     * Get total accuracy.
     * @return {number} - The ratio between the number of true predictions and total number of classifications ([0-1])
     */
    getAccuracy(): number;
    /**
     * Returns the element in the confusion matrix that corresponds to the given actual and predicted labels.
     * @param {any} actual - The true label
     * @param {any} predicted - The predicted label
     * @return {number} - The element in the confusion matrix
     */
    getCount(actual: any, predicted: any): number;
    /**
     * Compute the general prediction accuracy
     * @deprecated Use getAccuracy
     * @return {number} - The prediction accuracy ([0-1]
     */
    get accuracy(): number;
    /**
     * Compute the number of predicted observations
     * @deprecated Use getTotalCount
     * @return {number}
     */
    get total(): number;
}
//# sourceMappingURL=index.d.ts.map